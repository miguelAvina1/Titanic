# Inteligent Systems: Titanic Homewok.
#
# Authors:  Miguel Avina
#           Arturo Perez
# Date:     04/26/2020

# Importing files, code generated by RStudio "Import Dataset" tool.
library(readr)

train_data <- read_csv("data/train.csv")
test_data  <- read_csv("data/test.csv")

n_var <- ncol(train_data)  # Getting number of variables (same as columns)
sprintf("Number of variables of train data: %s", n_var)

n_numeric <- 0
n_categorical <- 0
for (column in colnames(train_data)) {
  if (class(train_data[[column]]) == "numeric") {
    n_numeric <- n_numeric + 1
  } else {
    n_categorical <- n_categorical + 1
  }
}
sprintf("No. of categorical variables: %s", n_categorical)
sprintf("No. of numerical variables %s", n_numeric)

str(train_data)  # Brief string representation of the data

# Removing:
# PassengerId: 1-891 series, not related to passenger
# Name: Altough we can correlate last names with travel classes or relationships between passengers, 
#       this will suppose great effort that may not be worth for the scope of this study.
# Ticket: This variable doen't give any relevan information.
# Embarked: All passengers were inside the ship at the crash. This is irrelevant.
library(dplyr)
train_data <- select(train_data, -PassengerId, -Name, -Ticket, -Embarked)







# Checking for missing data and storing the info.
# Creating the empty table to store the info:
missing_data <- matrix(data = 0, nrow = ncol(train_data), ncol = 3)
colnames(missing_data) <- c("Variable", "Occurrences", "Percentage")

missing_data[1:8, 1] <- colnames(train_data)
i <- 1
for (column in train_data) {
  missing_data[i, 2] <- sum(is.na(column))
  missing_data[i, 3] <- round(mean(is.na(column))*100, digits = 2)  
  i = i + 1
}
row_sub = apply(missing_data, 1, function(row) all(row !=0 )) # Checkig for zeros in rows
missing_data <- missing_data[row_sub,]                         # Subset to remove variables with complete data.

print("Missing data summary:")
print(missing_data)



# Analyzing 'Cabin' missing values #
# Since there is over 75% of missing data for the Cabin variable, it would be better to continue without it,
# However, we will check the percentage of ocurreces per level.
train_data$Cabin <- as.factor(train_data$Cabin)   # Convert it to categorical
sprintf("Cabin variable has %s levels", nlevels(train_data$Cabin))
prop.table(table(train_data$Cabin))
barplot(height = table(train_data$Cabin))
# We can observe that there are 147 levels for 891 observations, we can also observe that it would be impossible to join
# different levels to achive 10% in each of them since we don't have any information that will let us join them in a logical way.
# Furthermore, graphically we can observe that most of the Cabins only have one onccurence.
# Therefore, we will continue without this variable.
train_data <- select(train_data, -Cabin) # Removing Cabin variable


# Analyzing 'Age' missing values #
# Numerical and graphical analysis to determine which decision to take regarding missing data.
# Numerical Analysis
print(summary(train_data$Age))
sprintf("Std dev: %s", round(sd(train_data$Age, na.rm = TRUE), digits = 4))
boxplot(train_data$Age, ylab = "Years", xlab = 'Age')
hist(train_data$Age)
plot(density(train_data$Age, na.rm = TRUE))

# Verifying Age's correlation to the rest of the numerical variables
# Creating a subset first using listwise deletion
var_num <- data.frame(train_data$Age, train_data$Fare, train_data$SibSp, train_data$Parch)
var_num_corr <- na.omit(var_num)
cor(var_num_corr, method="pearson")
library(reshape2)
library(ggplot2)
tmp <- melt(data.matrix(cor(var_num_corr)))
ggplot(data=tmp, aes(x=Var1,y=Var2,fill=value)) + geom_tile()
# With the previous analysis we can observe that there isn't correlation between Age and any other variable,
# therefore, we can't remove it without losing important information. However, it is intersting to notice that
# there is a small inverse relationship between age and number of siblings and spouses aboard.

# We will now use stochastic regresion to fill missing values in 'Age'

library("mice")
library(dplyr, warn.conflicts = FALSE)

var_num <- data.frame(train_data$Age, train_data$Fare)  # We will use just Fare for this regressiom

imp <- mice(var_num, method = "norm.nob", m = 1, seed = 9)  # TODO: Research how to add min and max range to imputation by mice
var_num_regr <- complete(imp)

# Using mean imputation as reference for comparison
var_num_mean <- var_num
var_num_mean$train_data.Age[is.na(var_num_mean$train_data.Age)] <- median(var_num_mean$train_data.Age,na.rm=TRUE)
# Comparing summaries
summary(var_num_corr$train_data.Age) #  Original Age data, without NAs 
round(sd(var_num_corr$train_data.Age, na.rm = TRUE), digits = 4)
summary(var_num_regr$train_data.Age) #  Using regresion
round(sd(var_num_regr$train_data.Age, na.rm = TRUE), digits = 4)
summary(var_num_mean$train_data.Age) #  # Using mean imputation
round(sd(var_num_mean$train_data.Age, na.rm = TRUE), digits = 4)

# Graphical results
plot(density(var_num_corr$train_data.Age))  # Original Age data, without NAs
lines(density(var_num_regr$train_data.Age))  # Using regresion

plot(density(var_num_mean$train_data.Age))  # Using mean imputation
lines(density(var_num_corr$train_data.Age))  # Original Age data, without NAs

# From now on, we will keep with Age filled with stochastic regression
train_data$Age <- var_num_regr$train_data.Age


# Before we continue, we will change the PClass and survived variables from numerical to categorical
train_data$Survived <- ifelse(train_data$Survived==0, "Alive", "Dead")  # This changes int to char
train_data$Survived <- as.factor(train_data$Survived) # Effectively converted variable to categorical
train_data$Pclass <- as.factor(train_data$Pclass) 
train_data$Sex <- as.factor(train_data$Sex)

# We will verify distribution of data between categorical variable levels.
prop.table(table(train_data$Survived))
prop.table(table(train_data$Pclass))
prop.table(table(train_data$Sex))
# All previous variables have a good distribution of samples among their levels (>10% per level)

# We will convert SibSP and Parch to categorical variables also
train_data$SibSp <- as.factor(train_data$SibSp)
train_data$Parch <- as.factor(train_data$Parch)
prop.table(table(train_data$SibSp))
prop.table(table(train_data$Parch))
# Since some leves have under 10% of data in them, we will combine them as follows:
train_data$SibSp <- factor(train_data$SibSp, labels=c("NoSbSp","HaveSbSp","HaveSbSp","HaveSbSp","HaveSbSp","HaveSbSp","HaveSbSp"))
prop.table(table(train_data$SibSp))

train_data$Parch <- factor(train_data$Parch, labels = c("NoParch","HaveParch","HaveParch","HaveParch","HaveParch","HaveParch","HaveParch"))
prop.table(table(train_data$Parch))

# Know we have finishing preprocesing our data
data_A <- train_data                   # Data for model A
data_B <- select(train_data, -Age)     # Data for model B

# Dividing set in 80/20. Creating a random permutation
sz <- dim(data_A)[1]          
set.seed(6)
q<-order(runif(sz))          
mydata_raw <- data_A[q, ]    

train_data_A <- mydata_raw[1:floor(0.80*sz), ]      # 80% Training set   
test_data_A <- mydata_raw[(floor(0.80*sz)+1):(sz), ]
# Verifying uniformity of proportions in each subset:
prop.table(table(train_data_A$Survived))
prop.table(table(test_data_A$Survived))
prop.table(table(train_data$Survived))



library(rpart.plot)

fit <- rpart(Survived ~ ., data=train_data_A, method='class')
rpart.plot(fit, extra=106)  # 106:binary; 104:multiclass

# Predictions
pred_test = predict(fit, test_data_A, type='class')

table(test_data_A$Survived, pred_test)



