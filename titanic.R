# Inteligent Systems: Titanic Homewok.
#
# Authors:  Miguel Avina
#           Arturo Perez
# Date:     04/26/2020

# Importing files, code generated by RStudio "Import Dataset" tool.
library(readr)

train_data <- read_csv("data/train.csv")
test_data  <- read_csv("data/test.csv")

n_var <- ncol(train_data)  # Getting number of variables (same as columns)
sprintf("Number of variables of train data: %s", n_var)

n_numeric <- 0
n_categorical <- 0
for (column in colnames(train_data)) {
  if (class(train_data[[column]]) == "numeric") {
    n_numeric <- n_numeric + 1
  } else {
    n_categorical <- n_categorical + 1
  }
}
sprintf("No. of categorical variables: %s", n_categorical)
sprintf("No. of numerical variables %s", n_numeric)

str(train_data)  # Brief string representation of the data

# Removing:
# PassengerId: 1-891 series, not related to passenger
# Name: Altough we can correlate last names with travel classes or relationships between passengers, 
#       this will suppose great effort that may not be worth for the scope of this study.
# Ticket: This variable doen't give any relevan information.
# Embarked: All passengers were inside the ship at the crash. This is irrelevant.
library(dplyr)
train_data <- select(train_data, -PassengerId, -Name, -Ticket, -Embarked)







# Checking for missing data and storing the info.
# Creating the empty table to store the info:
missing_data <- matrix(data = 0, nrow = ncol(train_data), ncol = 3)
colnames(missing_data) <- c("Variable", "Occurrences", "Percentage")

missing_data[1:8, 1] <- colnames(train_data)
i <- 1
for (column in train_data) {
  missing_data[i, 2] <- sum(is.na(column))
  missing_data[i, 3] <- round(mean(is.na(column))*100, digits = 2)  
  i = i + 1
}
row_sub = apply(missing_data, 1, function(row) all(row !=0 )) # Checkig for zeros in rows
missing_data <- missing_data[row_sub,]                         # Subset to remove variables with complete data.

print("Missing data summary:")
print(missing_data)




# Analyzing 'Cabin' missing values #
# Since there is over 75% of missing data for the Cabin variable, it would be better to continue without it,
# However, we will check the percentage of ocurreces per level.
train_data$Cabin <- as.factor(train_data$Cabin)   # Convert it to categorical
sprintf("Cabin variable has %s levels", nlevels(train_data$Cabin))
prop.table(table(train_data$Cabin))
barplot(height = table(train_data$Cabin))
# We can observe that there are 147 levels for 891 observations, we can also observe that it would be impossible to join
# different levels to achive 10% in each of them since we don't have any information that will let us join them in a logical way.
# Furthermore, graphically we can observe that most of the Cabins only have one onccurence.
# Therefore, we will continue without this variable.
train_data <- select(train_data, -Cabin) # Removing Cabin variable





# Analyzing 'Age' missing values #
# Numerical and graphical analysis to determine which decision to take regarding missing data.
# Numerical Analysis
print(summary(train_data$Age))
sprintf("Std dev: %s", round(sd(train_data$Age, na.rm = TRUE), digits = 4))
boxplot(train_data$Age, ylab = "Years", xlab = 'Age')
hist(train_data$Age)
plot(density(train_data$Age, na.rm = TRUE))

# Verifying Age's correlation to the rest of the numerical variables
# Creating a subset first using listwise deletion
var_num <- data.frame(train_data$Age, train_data$Fare, train_data$SibSp, train_data$Parch)
var_num_corr <- na.omit(var_num)
cor(var_num_corr, method="pearson")
library(reshape2)
library(ggplot2)
tmp <- melt(data.matrix(cor(var_num_corr)))
ggplot(data=tmp, aes(x=Var1,y=Var2,fill=value)) + geom_tile()
# With the previous analysis we can observe that there isn't correlation between Age and any other variable,
# therefore, we can't remove it without losing important information. However, it is intersting to notice that
# there is a small inverse relationship between age and number of siblings and spouses aboard.

# We will now use stochastic regresion to fill missing values in 'Age'

library("mice")
library(dplyr, warn.conflicts = FALSE)

var_num <- data.frame(train_data$Age, train_data$Fare)  # We will use just Fare for this regressiom

imp <- mice(var_num, method = "norm.nob", m = 1, seed = 9)  # TODO: Research how to add min and max range to imputation by mice
var_num_regr <- complete(imp)

# Using mean imputation as reference for comparison
var_num_mean <- var_num
var_num_mean$train_data.Age[is.na(var_num_mean$train_data.Age)] <- median(var_num_mean$train_data.Age,na.rm=TRUE)
# Comparing summaries
summary(var_num_corr$train_data.Age) #  Original Age data, without NAs 
round(sd(var_num_corr$train_data.Age, na.rm = TRUE), digits = 4)
summary(var_num_regr$train_data.Age) #  Using regresion
round(sd(var_num_regr$train_data.Age, na.rm = TRUE), digits = 4)
summary(var_num_mean$train_data.Age) #  # Using mean imputation
round(sd(var_num_mean$train_data.Age, na.rm = TRUE), digits = 4)

# Graphical results
plot(density(var_num_corr$train_data.Age))  # Original Age data, without NAs
lines(density(var_num_regr$train_data.Age))  # Using regresion

plot(density(var_num_mean$train_data.Age))  # Using mean imputation
lines(density(var_num_corr$train_data.Age))  # Original Age data, without NAs

# From now on, we will keep with Age filled with stochastic regression
train_data$Age <- var_num_regr$train_data.Age





# Before we continue, we will change the PClass and survived variables from numerical to categorical
train_data$Survived <- ifelse(train_data$Survived==1, "Alive", "Dead")  # This changes int to char
train_data$Survived <- as.factor(train_data$Survived) # Effectively converted variable to categorical
train_data$Pclass <- as.factor(train_data$Pclass) 
train_data$Sex <- as.factor(train_data$Sex)

# We will verify distribution of data between categorical variable levels.
prop.table(table(train_data$Survived))
prop.table(table(train_data$Pclass))
prop.table(table(train_data$Sex))
# All previous variables have a good distribution of samples among their levels (>10% per level)

# We will convert SibSP and Parch to categorical variables also
train_data$SibSp <- as.factor(train_data$SibSp)
train_data$Parch <- as.factor(train_data$Parch)
prop.table(table(train_data$SibSp))
prop.table(table(train_data$Parch))
# Since some leves have under 10% of data in them, we will combine them as follows:
train_data$SibSp <- factor(train_data$SibSp, labels=c("NoSbSp","HaveSbSp","HaveSbSp","HaveSbSp","HaveSbSp","HaveSbSp","HaveSbSp"))
prop.table(table(train_data$SibSp))

train_data$Parch <- factor(train_data$Parch, labels = c("NoParch","HaveParch","HaveParch","HaveParch","HaveParch","HaveParch","HaveParch"))
prop.table(table(train_data$Parch))


 #train_data <- read_csv("data/train.csv")
 #train_data <- select(train_data, -PassengerId, -Name, -Ticket, -Embarked)







# Is there any kind of correlation between fare and PClass (it should be, right?)
n_PClass <- as.integer(train_data$Pclass)

in_corr <- data.frame(n_PClass, train_data$Fare)
cor(in_corr, method="pearson")
library(reshape2)
library(ggplot2)
tmp <- melt(data.matrix(cor(in_corr)))
ggplot(data=tmp, aes(x=Var1,y=Var2,fill=value)) + geom_tile()
# Just 0.5 of correlation, not enough to remove variable





library("timereg")

# We will convert Fare to a categorical variable with 4 levels, based on the quartiles
train_data$Fare = qcut(train_data$Fare, cuts = 4)



# Percentile_00  <- min(train_data$Fare)
# Percentile_33  <- unname(quantile(train_data$Fare, 0.33333))
# Percentile_67  <- unname(quantile(train_data$Fare, 0.66667))
# Percentile_100 <- max(train_data$Fare)
# lab_1 = sprintf("Less than %s", Percentile_33)
# lab_2 = sprintf("Over %s, less than %s", Percentile_33, Percentile_67)
# lab_3 = sprintf("Over %s", Percentile_67)
# 
# train_data$Fare = cut(x = train_data$Fare, breaks = c(Percentile_00, Percentile_33, Percentile_67, Percentile_100),
#              include.lowest = TRUE, right = TRUE, labels = c(lab_1, lab_2, lab_3))
# 
# 
# 

# We will convert Age to a categorical variable with 3 levels, based on nothing solid
# nino  <- 5  # Less than
# adulto  <- 45  # Less than
# #adulto  <- max(train_data$Age)  # Less than
# mayor <- max(train_data$Age)  # Less than
# 
# 
# train_data$Age = cut(x = train_data$Age, breaks = c(0, nino, adulto, mayor),
#                       include.lowest = TRUE, right = TRUE, labels = c("Nino", "Adulto", "Mayor"))
# 

#train_data$Age = qcut(train_data$Age, cuts = 4)





# END OF PREPROCESSING #

# Obtainign a decision tree based using our model A
data_A <- train_data                   # Data for model A
data_B <- select(train_data, -Age)     # Data for model B

# Dividing set in 80/20. Creating a random permutation
sz <- dim(data_A)[1]          
set.seed(6)
q<-order(runif(sz))          
mydata_raw_A <- data_A[q, ]    

train_data_A <- mydata_raw_A[1:floor(0.70*sz), ]      # 80% Training set   
test_data_A <- mydata_raw_A[(floor(0.70*sz)+1):(sz), ]

# Verifying uniformity of proportions in each subset:
prop.table(table(train_data_A$Survived))
prop.table(table(test_data_A$Survived))
prop.table(table(train_data$Survived))

# classification tree

library(rpart.plot)

fit_A <- rpart(Survived ~ ., data=train_data_A, method='class')
rpart.plot(fit_A, extra = 106)  # 106:binary; 104:multiclass


# Predictions
pred_test_A = predict(fit, test_data_A, type='class')

# Generating confusion matrix and other stats
library(caret)
confusionMatrix(pred_test_A, test_data_A$Survived, positive = NULL, dnn = c("Prediction", "Reference"))


############################
# Repeating 5 times previous process, with different seeds, storing the outputs of each iteration

seeds_i <- c(10, 33, 50, 89, 100)  # seed 6 was used in first example before theese 5 iterations

trees_A <- vector("list", length(seeds_i))    # Empty list were we are going to save info for each iteration
matrices_A <- vector("list", length(seeds_i))   # Empty list were we are going to save info for each iteration

index <- 1
for (seed in seeds_i){
  set.seed(seed)
  q<-order(runif(sz))          
  mydata_raw_A <- data_A[q, ]    
  
  train_data_A <- mydata_raw_A[1:floor(0.70*sz), ]      # 80% Training set   
  test_data_A <- mydata_raw_A[(floor(0.70*sz)+1):(sz), ]
  
  trees_A[[index]] <- rpart(Survived ~ ., data=train_data_A, method='class')   # Saving the result of this iteration
  rpart.plot(fit_A, extra = 106)  # 106:binary; 104:multiclass
  
  pred_test = predict(fit_A, test_data_A, type='class')
  matrices_A[[index]] <- confusionMatrix(pred_test_A, test_data_A$Survived, positive = NULL, dnn = c("Prediction", "Reference")) # Saving the result of this iteration
  

  index <- index + 1
}

# trees and matrices are lists of five lists, we can access each list by trees[1] (info from 1st iteratioon) and matrices[3] conf.matrix from 3rd iteration
# Getting average confusion matrix
VP <- 0
FN <- 0
FP <- 0
VN <- 0
for (matrix in matrices_A) {   # Meaning of table's indexes: 1:VP; 2:FN; 3:FP; 4:VN
  VP <- VP + matrix$table[1]
  FN <- FN + matrix$table[2]
  FP <- FP + matrix$table[3]
  VN <- VN + matrix$table[4]
}
VP <- round(VP/length(seeds_i))
FN <- round(FN/length(seeds_i))
FP <- round(FP/length(seeds_i))
VN <- round(VN/length(seeds_i))

print(matrix(c(VP, FN, FP, VN), nrow=2, dimnames = list(c("Alive", "Dead"), c("Alive", "Dead"))))




# set.seed(6). Seeds user for each of the HW iterations: {6, 10, 33, 50, 89, 100}

# Dividing set in 80/20. Creating a random permutation
sz <- dim(data_B)[1]          
set.seed(6)
q<-order(runif(sz))          
mydata_raw_B <- data_B[q, ]    

train_data_B <- mydata_raw_B[1:floor(0.60*sz), ]      # 80% Training set   
test_data_B <- mydata_raw_B[(floor(0.60*sz)+1):(sz), ]

# Verifying uniformity of proportions in each subset:
prop.table(table(train_data_B$Survived))
prop.table(table(test_data_B$Survived))
prop.table(table(train_data$Survived))

# classification tree

library(rpart.plot)

fit_B <- rpart(Survived ~ ., data=train_data_B, method='class')
rpart.plot(fit_B, extra = 106)  # 106:binary; 104:multiclass


# Predictions
pred_test_B = predict(fit_B, test_data_B, type='class')

# Generating confusion matrix and other stats
library(caret)
confusionMatrix(pred_test_B, test_data_B$Survived, positive = NULL, dnn = c("Prediction", "Reference"))


############################
# Repeating 5 times previous process, with different seeds, storing the outputs of each iteration

seeds_i <- c(10, 33, 50, 89, 100)  # seed 6 was used in first example before theese 5 iterations

trees_B <- vector("list", length(seeds_i))    # Empty list were we are going to save info for each iteration
matrices_B <- vector("list", length(seeds_i))   # Empty list were we are going to save info for each iteration

index <- 1
for (seed in seeds_i){
  set.seed(seed)
  q<-order(runif(sz))          
  mydata_raw_B <- data_B[q, ]    
  
  train_data_B <- mydata_raw_B[1:floor(0.60*sz), ]      # 80% Training set   
  test_data_B <- mydata_raw_B[(floor(0.60*sz)+1):(sz), ]
  
  trees_B[[index]] <- rpart(Survived ~ ., data=train_data_B, method='class')   # Saving the result of this iteration
  rpart.plot(fit_B, extra = 106)  # 106:binary; 104:multiclass
  
  pred_test_B = predict(fit_B, test_data_B, type='class')
  matrices_B[[index]] <- confusionMatrix(pred_test_B, test_data_B$Survived, positive = NULL, dnn = c("Prediction", "Reference")) # Saving the result of this iteration
  
  
  index <- index + 1
}

# trees and matrices are lists of five lists, we can access each list by trees[1] (info from 1st iteratioon) and matrices[3] conf.matrix from 3rd iteration
# Getting average confusion matrix
VP <- 0
FN <- 0
FP <- 0
VN <- 0
for (matrix in matrices_B) {   # Meaning of table's indexes: 1:VP; 2:FN; 3:FP; 4:VN
  VP <- VP + matrix$table[1]
  FN <- FN + matrix$table[2]
  FP <- FP + matrix$table[3]
  VN <- VN + matrix$table[4]
}
VP <- round(VP/length(seeds_i))
FN <- round(FN/length(seeds_i))
FP <- round(FP/length(seeds_i))
VN <- round(VN/length(seeds_i))

print(matrix(c(VP, FN, FP, VN), nrow=2, dimnames = list(c("Alive", "Dead"), c("Alive", "Dead"))))



